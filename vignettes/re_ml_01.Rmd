---
title: "Report Exercise Chapter 10"
author: "Ken RÃ¶lli"
output:
  html_document:
    toc: true
editor_options: 
  chunk_output_type: console
---

For convenience we load again some packages.
```{r message = FALSE}
library(tidyverse)
library(recipes)
library(caret)
library(recipes)
library(rsample)
```

Then we insert the code provided in the tutorial. Some parts are now as functions in a
separate `.R` script in the `/R` directory.
```{r message = FALSE}
source(here::here("R", "prepare_df.R"))

daily_fluxes <- prepare_df()
daily_fluxes |> head()
```

```{r}
# Data splitting
set.seed(1982)  # for reproducibility
split <- rsample::initial_split(daily_fluxes, prop = 0.7, strata = "VPD_F")
daily_fluxes_train <- rsample::training(split)
daily_fluxes_test <- rsample::testing(split)

# Model and pre-processing formulation, use all variables but LW_IN_F
pp <- recipes::recipe(GPP_NT_VUT_REF ~ SW_IN_F + VPD_F + TA_F, 
                      data = daily_fluxes_train |> drop_na()) |> 
  recipes::step_BoxCox(recipes::all_predictors(), -TA_F) |> # TODO: Check why we should try to boxcox transform TA_F -> it also contains negative values!
  recipes::step_center(recipes::all_numeric(), -recipes::all_outcomes()) |>
  recipes::step_scale(recipes::all_numeric(), -recipes::all_outcomes())

# Fit linear regression model
mod_lm <- caret::train(
  pp, 
  data = daily_fluxes_train |> drop_na(), 
  method = "lm",
  trControl = caret::trainControl(method = "none"),
  metric = "RMSE"
)

# Fit KNN model
mod_knn <- caret::train(
  pp, 
  data = daily_fluxes_train |> drop_na(), 
  method = "knn",
  trControl = caret::trainControl(method = "none"),
  tuneGrid = expand.grid(k = 8),
  metric = "RMSE"
)
```

```{r}
source(here::here("R", "eval_model.R"))

# linear regression model
eval_model(mod = mod_lm, df_train = daily_fluxes_train, df_test = daily_fluxes_test)

# knn model
eval_model(mod = mod_knn, df_train = daily_fluxes_train, df_test = daily_fluxes_test)
```

# Interpretation

## Why is the difference between the evaluation on the training and the test set larger for the KNN model than for the linear regression model?
The linear model is very simple and has only two parameters that are used for prediction:
slope and intercept.
Thanks to its simplicity, it often leads to similar performances on data used for training and on new data as we observed here.
On the other hand, the KNN model is more flexible because it takes the average prediction of the k (in our case 8) predictions closest to the observation we want to predict.
It's outcome depends heavily on the data that was used for training thus its performance often decreases quite a bit on new data (like the test set).

## Why is the does the evaluation on the test set indicate a better model performance of the KNN model than the linear regression model?
Even though the $R^2$ dropped more compared to the linear model it is still higher on the test set than the $R^2$ value of the linear model (0.63 vs. 0.60)

## How would you position the KNN and the linear regression model along the spectrum of the bias-variance trade-off?
KNN:
- high variance
- low bias

Linear regression:
- low variance
- high bias

Above observations in the previous two questions agree with this.

# The role of k

## k approaches N
If $k$ approaches $N$, every prediction will be made by averaging over (almost) all training target values, because the $k$ nearest neighbors will be (almost) all the training observations.

### Training
Thus, I expect that the $R^2$ value will decrease (close) to $0$ because it tells how much better our fitted values $\hat Y$ are than just taking the average of the target $\bar Y$ as predictions (as written in Tutorial).

Based on the formula and the resulting prediction for $k=N$ $\hat Y_j = \bar Y$ for all $j= 1, \dots, N$ I expect it to increase to
$$\text{MAE} = \frac{1}{n}\sum_i \lvert Y_i - \bar Y\rvert,$$ 


### Testing
I expect that the $R^2$ value will decrease, but not necessarily to exactly zero, it could be even decrease to something lower than zero.
I also expect that the $\text{MAE}$ increases because the model will make less accurate predictions.

## k approaches 1
On the other hand if $k$ approaches $1$ every prediction will be made by averaging over (almost) 1 training target value, because the $k \to 1$ nearest neighbors will be the training observations that had the closest covariate values compared to the one we want to predict, so it will almost surely be the  

### Training
Thus, I expect that the $R^2$ value will be close to $1$ because 
$$\sum _i (Y_i - \hat Y_i)^2$$ will be (close to) $0$ and therefore 
$$R^2 = 1 - \frac{\sum _i (Y_i - \hat Y_i)^2}{\sum _i (Y_i - \bar Y_i)^2} \approx 1 - 0 = 1$$

I expect the $\text{MAE}$ to decrease to approximately $0$ because $\sum_i \lvert \hat Y_i - Y_i \rvert$ will be close to $0$ (exactly $0$ if there are no two observations with exactly the same covariate values).

### Testing
I expect that the $R^2$ will decrease, because the model will be fitted to the training data and its noise, so it generalizes less well to new data than for $0 << k << N$.
For the MAE I think it increases because it makes less accurate predictions. 

## Hypothesis testing
```{r}
set.seed(1)  # for reproducibility
split <- rsample::initial_split(daily_fluxes |> drop_na(), prop = 0.7, strata = "VPD_F")
daily_fluxes_train <- rsample::training(split)
daily_fluxes_test <- rsample::testing(split)

# Model and pre-processing formulation, use all variables but LW_IN_F
pp <- recipes::recipe(GPP_NT_VUT_REF ~ SW_IN_F + VPD_F + TA_F, 
                      data = daily_fluxes_train) |> 
  recipes::step_BoxCox(recipes::all_predictors(), -TA_F) |> # TODO: Check why we should try to boxcox transform TA_F -> it also contains negative values!
  recipes::step_center(recipes::all_numeric(), -recipes::all_outcomes()) |>
  recipes::step_scale(recipes::all_numeric(), -recipes::all_outcomes())

metrics_train <- tibble(k = numeric(), R2 = numeric(), MAE = numeric())
metrics_test <- tibble(k = numeric(), R2 = numeric(), MAE = numeric())
k_vec <- c(seq(1, 5, by = 1), seq(8, 20, by = 3), seq(30, 100, 10)) # TODO: Check why higher k throws "too many ties in knn" error.
for(k in k_vec) {
  mod_knn <- caret::train(
  pp, 
  data = daily_fluxes_train, 
  method = "knn",
  trControl = caret::trainControl(method = "none"),
  tuneGrid = expand.grid(k = k),
  metric = "RMSE"
)
  
  daily_fluxes_train$train_predict <- predict(mod_knn, newdata = daily_fluxes_train)
  daily_fluxes_test$test_predict <- predict(mod_knn, newdata = daily_fluxes_test)
  m_train <- daily_fluxes_train |> yardstick::metrics(GPP_NT_VUT_REF, train_predict)
  m_test <- daily_fluxes_test |> yardstick::metrics(GPP_NT_VUT_REF, test_predict)
  rsq_train <- m_train |> 
    filter(.metric == "rsq") |> 
    pull(.estimate)
  rsq_test <- m_test |> 
    filter(.metric == "rsq") |> 
    pull(.estimate)
  
  mae_train <- m_train |> 
    filter(.metric == "mae") |> 
    pull(.estimate)
  mae_test <- m_test |> 
    filter(.metric == "mae") |> 
    pull(.estimate)
  metrics_train <- metrics_train |> add_row(k = k, R2 = rsq_train, MAE = mae_train)
  metrics_test <- metrics_test |> add_row(k = k, R2 = rsq_test, MAE = mae_test)
}

metrics_train
metrics_test
```

### Comment
The prediction were quite accurate, however I could not increase $k$ really close to $N$ because it lead to an error...

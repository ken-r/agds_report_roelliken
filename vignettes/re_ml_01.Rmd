---
title: "Report Exercise Chapter 10"
author: "Ken RÃ¶lli"
output:
  html_document:
    toc: true
editor_options: 
  chunk_output_type: console
knit: (function(input, ...) {rmarkdown::render(input, output_dir = "../html")})
---

For convenience we load again some packages.
```{r message = FALSE}
library(tidyverse)
library(recipes)
library(caret)
library(rsample)
```

Then we insert the code provided in the tutorial. Some parts are now as functions in a
separate `.R` script in the `/R` directory.
```{r message = FALSE}
source(here::here("R", "prepare_df.R"))

daily_fluxes <- prepare_df(here::here("data", "FLX_CH-Dav_FLUXNET2015_FULLSET_DD_1997-2014_1-3.csv"))
daily_fluxes |> head()
```

```{r}
source(here::here("R", "recipe_daily_fluxes.R"))

# Data splitting
set.seed(1982)  # for reproducibility
split <- rsample::initial_split(daily_fluxes, prop = 0.7, strata = "VPD_F")
daily_fluxes_train <- rsample::training(split)
daily_fluxes_test <- rsample::testing(split)

pp <- recipe_daily_fluxes(daily_fluxes_train |> drop_na())

# Fit linear regression model
mod_lm <- caret::train(
  pp, 
  data = daily_fluxes_train |> drop_na(), 
  method = "lm",
  trControl = caret::trainControl(method = "none"),
  metric = "RMSE"
)

# Fit KNN model
mod_knn <- caret::train(
  pp, 
  data = daily_fluxes_train |> drop_na(), 
  method = "knn",
  trControl = caret::trainControl(method = "none"),
  tuneGrid = expand.grid(k = 8),
  metric = "RMSE"
)
```

```{r}
source(here::here("R", "eval_model.R"))

# linear regression model
eval_model(mod = mod_lm, df_train = daily_fluxes_train, df_test = daily_fluxes_test)

# knn model
eval_model(mod = mod_knn, df_train = daily_fluxes_train, df_test = daily_fluxes_test)
```

# Interpretation

## Why is the difference between the evaluation on the training and the test set larger for the KNN model than for the linear regression model?

The linear model is very simple and has only two parameters that are used for prediction:
slope and intercept.
Thanks to its simplicity, it often leads to similar performances on data used for training and on new data as we observed here.
On the other hand, the KNN model is more flexible because it takes the average prediction of the k (in our case 8) predictions closest to the observation we want to predict.
It's outcome depends heavily on the data that was used for training thus its performance often decreases quite a bit on new data (like the test set).

## Why is the does the evaluation on the test set indicate a better model performance of the KNN model than the linear regression model?

Even though the $R^2$ dropped more compared to the linear model it is still higher on the test set than the $R^2$ value of the linear model (0.63 vs. 0.60)

## How would you position the KNN and the linear regression model along the spectrum of the bias-variance trade-off?

KNN:
- high variance
- low bias

Linear regression:
- low variance
- high bias

Above observations in the previous two questions agree with this.

## Visualize temporal variations
At first, we need to add the predictions to the daily fluxes dataset.
```{r}
daily_fluxes_pred_data <- daily_fluxes |> 
  drop_na()

daily_fluxes_pred_data <- daily_fluxes_pred_data |> 
  mutate(
    pred_lm = predict(mod_lm, newdata = daily_fluxes_pred_data),
    pred_knn = predict(mod_knn, newdata = daily_fluxes_pred_data)
  )
```

```{r}
# Observed data plot
gpp_obs <- ggplot(
  data = daily_fluxes_pred_data,
  aes(x = TIMESTAMP, y = GPP_NT_VUT_REF)) +
  geom_line() +
  ylim(-1, 10) +
  labs(title = "Observed data",
       x = "Date",
       y = expression(paste("GPP (", mu, "mol CO"[2], "m"^-2, "s"^-1, ")"))
       )
nrow(daily_fluxes_pred_data)
nrow(daily_fluxes_pred_data |> drop_na())
# Linear model prediction plot
gpp_lm <- ggplot(
  data = daily_fluxes_pred_data,
  aes(x = TIMESTAMP, y = pred_lm)) +
  geom_line() +
  ylim(-1, 10) +
  labs(title = "Linear model predictions",
       x = "Date",
       y = expression(paste("GPP (", mu, "mol CO"[2], "m"^-2, "s"^-1, ")"))
  )

# Residual plot LM
res_lm <- ggplot(
  data = daily_fluxes_pred_data,
  aes(x = TIMESTAMP, y = GPP_NT_VUT_REF - pred_lm)) +
  geom_line() +
  ylim(-6, 7.5) +
  geom_hline(yintercept = 0, linetype = 'dashed', color = "green") +
  labs(title = "Linear model residuals", 
       x = "Date", 
       y = expression(paste("Residuals (", mu, "mol CO"[2], "m"^-2, "s"^-1, ")"))
       )

# KNN prediction plot
gpp_knn <- ggplot(
  data = daily_fluxes_pred_data,
  aes(x = TIMESTAMP, y = pred_knn)) +
  geom_line() +
  ylim(-1, 10) +
  labs(title = "KNN model predictions",
       x = "Date",
       y = expression(paste("GPP (", mu, "mol CO"[2], "m"^-2, "s"^-1, ")"))
       )

# Residual plot LM
res_knn <- ggplot(
  data = daily_fluxes_pred_data,
  aes(x = TIMESTAMP, y = GPP_NT_VUT_REF - pred_knn)) +
  geom_line() +
  ylim(-6, 7.5) +
  geom_hline(yintercept = 0, linetype = 'dashed', color = "green") +
  labs(title = "KNN model residuals", 
       x = "Date", 
       y = expression(paste("Residuals (", mu, "mol CO"[2], "m"^-2, "s"^-1, ")"))
       )

cowplot::plot_grid(gpp_obs, gpp_lm, res_lm, gpp_obs, gpp_knn, res_knn, nrow = 3, ncol = 2, byrow = FALSE)
```

**Comment**
We see that a lot of data from 2005 is missing.
Otherwise the observed and modeled plots look very similar. Also the linear model and knn predictions plot look alike.

### Further analysis
In order to compare the models, one could take a look at the average GPP level per day (from 1. January to 31. December) and look whether there are differences in the GPP level predicted by LM/KNN. In such a plot, the differences should be easier to spot than on a plot that contains all dates between 1997 and 2015.
Due to time reasons, we don't do it here.


# The role of k

## $k \to N$
If $k$ approaches $N$, every prediction will be made by averaging over (almost) all training target values, because the $k$ nearest neighbors will be (almost) all the training observations.

### Training
Thus, I expect that the $R^2$ value will decrease (close) to $0$ because it tells how much better our fitted values $\hat Y$ are than just taking the average of the target $\bar Y$ as predictions (as written in Tutorial).

Based on the formula and the resulting prediction for $k=N$ $\hat Y_j = \bar Y$ for all $j= 1, \dots, N$ I expect it to increase to
$$\text{MAE} = \frac{1}{n}\sum_i \lvert Y_i - \bar Y\rvert,$$ 


### Testing
I expect that the $R^2$ value will decrease, but not necessarily to exactly zero, it could be even decrease to something lower than zero.
I also expect that the $\text{MAE}$ increases because the model will make less accurate predictions.

## $k \to 1$
On the other hand if $k$ approaches $1$ every prediction will be made by averaging over (almost) 1 training target value, because the $k \to 1$ nearest neighbors will be the training observations that had the closest covariate values compared to the one we want to predict, so it will almost surely be the  

### Training
Thus, I expect that the $R^2$ value will be close to $1$ because 
$$\sum _i (Y_i - \hat Y_i)^2$$ will be (close to) $0$ and therefore 
$$R^2 = 1 - \frac{\sum _i (Y_i - \hat Y_i)^2}{\sum _i (Y_i - \bar Y_i)^2} \approx 1 - 0 = 1$$

I expect the $\text{MAE}$ to decrease to approximately $0$ because $\sum_i \lvert \hat Y_i - Y_i \rvert$ will be close to $0$ (exactly $0$ if there are no two observations with exactly the same covariate values).

### Testing
I expect that the $R^2$ will decrease, because the model will be fitted to the training data and its noise, so it generalizes less well to new data than for $0 << k << N$.
For the MAE I think it increases because it makes less accurate predictions. 

## Hypothesis testing
We will work with a new train and test split where we already remove NAN values before splitting (for simplicity)
```{r}
source(here::here("R", "eval_model_metrics.R"))

set.seed(06062023)  # for reproducibility
split <- rsample::initial_split(daily_fluxes |> drop_na(), prop = 0.7, strata = "VPD_F")
daily_fluxes_train <- rsample::training(split)
daily_fluxes_test <- rsample::testing(split)

# Model and pre-processing formulation, use all variables but LW_IN_F
pp <- daily_fluxes_train |> recipe_daily_fluxes()

metrics_train <- tibble(k = numeric(), R2 = numeric(), MAE = numeric())
metrics_test <- tibble(k = numeric(), R2 = numeric(), MAE = numeric())
k_vec <- c(seq(1, 5, by = 1), seq(8, 20, by = 2), seq(25, 50, 5), seq(100, 400, 100), 499) # TODO: Check why higher k throws "too many ties in knn" error.
for(k in k_vec) {
  mod_knn <- caret::train(
  pp, 
  data = daily_fluxes_train, 
  method = "knn",
  trControl = caret::trainControl(method = "none"),
  tuneGrid = expand.grid(k = k),
  metric = "MAE"
)

  train_performance <- eval_model_metrics(mod_knn, daily_fluxes_train)
  test_performance <- eval_model_metrics(mod_knn, daily_fluxes_test)
  metrics_train <- metrics_train |> add_row(k = k, R2 = train_performance$R2, MAE = train_performance$MAE)
  metrics_test <- metrics_test |> add_row(k = k, R2 = test_performance$R2, MAE = test_performance$MAE)
}

metrics_train
metrics_test
metrics_test |> ggplot(aes(x = factor(k_vec), y = MAE)) +
  geom_point() +
  labs(x = "k", y = "MAE") +
  ggtitle("MAE on test set for different k")
```

### Evaluation of $k \to n$
We did not consider $k = n$ but $k = 499$, whereas the test set has `r nrow(daily_fluxes_test)` observations. 
```{r}
metrics_train |> tail(1)
metrics_test |> tail(1)
```

**Comment**
The predictions were quite accurate, however I could not increase $k$ really close to $N$ because it lead to an error "too many ties ...". But I expect that the results are similar but they would be even more extreme when $k$ is closer to $N$.

### Evaluation of $k \to 1$
```{r}
metrics_train |> head(1)
metrics_test |> head(1)
```

**Comment** As expected, the training performance is perfect, but the test performance
isn't. The model is overfit.

## Optimal k
Let's find some optimal k using the function we have created in `/R/eval_knn.R` as described in the task.
```{r}
source(here::here("R", "eval_knn.R"))

k_vec <- c(1:30, seq(33, 51, 3))
mae_test <- k_vec |> sapply(FUN = function(k) eval_knn(k, daily_fluxes_train, daily_fluxes_test))
opt_k <- k_vec[which.min(mae_test)]
```

So we see that the optimal k found was `r opt_k`. As expected, it should not be too close to 1 but also not too large.